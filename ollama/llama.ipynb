{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch accelerate langchain faiss-cpu bitsandbytes\n",
    "!pip install huggingface_hub\n",
    "!pip install pydantic==1.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the goal of life? If the goal of life is to see God in all his works, then we are to learn how to sit at the Well (the place where verse 14 tells us to be) alongside Him, the creator and saviour of the universe. Because itCurrent Time:03:06PMEarly Childhood:Build Your Own House25Then they said, Lord, did not one also die for us in Baptist addresses from seven different sources for every chapter, their Decrees? Than they all gather to hear\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Set the environment variable to disable parallelism in tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Retrieve the token from environment variables\n",
    "api_token = os.getenv(\"HF_API_KEY\")  # Ensure this variable is set in your environment\n",
    "from huggingface_hub import login\n",
    "\n",
    "# login(token=api_token, add_to_git_credential=True)\n",
    "inference = InferenceClient(model=\"meta-llama/Llama-3.2-1B\", token=api_token)\n",
    "# Define your prompt or input text without using [MASK]\n",
    "prompt = \"What is the goal of life?\"\n",
    "\n",
    "# Make the request to the model for inference\n",
    "response = inference.post(json={\"inputs\": prompt})\n",
    "\n",
    "import json\n",
    "\n",
    "# Convert the response from bytes to JSON\n",
    "response_json = json.loads(response)\n",
    "\n",
    "# Extract and print the generated text\n",
    "generated_text = response_json[0][\"generated_text\"]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pydantic\n",
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "response = pipe(\"The key to life is\", max_length=50)  # Adjust max_length as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key to life is to make the right decisions. And to make the right decisions, you need to understand the right information.\n",
      "At the heart of the decision-making process is information. Information is the fuel that keeps the engine of life running.\n"
     ]
    }
   ],
   "source": [
    "# print(response)\n",
    "generated_text = response[0]['generated_text']\n",
    "\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
