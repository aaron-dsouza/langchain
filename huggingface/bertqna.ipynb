{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers langchain\n",
    "!pip install langchain\n",
    "!pip install pydantic==1.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain transformers torch\n",
    "!pip install pydantic==1.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show langchain\n",
    "!pip show pydantic\n",
    "!pip show transformers\n",
    "!pip show torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, pipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the pipeline with LangChain's HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=qa_pipeline,\n",
    "    model_kwargs={\"temperature\": 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Hugging Face is creating a tool that democratizes AI.\",\n",
    "    \"BERT stands for Bidirectional Encoder Representations from Transformers.\"\n",
    "]\n",
    "\n",
    "# Initialize embeddings using HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='bert-base-uncased')\n",
    "\n",
    "# Create a FAISS vector store from the documents\n",
    "vectorstore = FAISS.from_texts(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for LLMChain\nverbose\n  Input should be a valid boolean [type=bool_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.9/v/bool_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever(search_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})  \u001b[38;5;66;03m# Retrieve top 1 relevant document\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the RetrievalQA chain\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      6\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m      7\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# \"stuff\" is a simple chain type that stuffs all retrieved docs into the prompt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mretriever,\n\u001b[1;32m      9\u001b[0m     return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:115\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 115\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m load_qa_chain(\n\u001b[1;32m    116\u001b[0m     llm, chain_type\u001b[38;5;241m=\u001b[39mchain_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_chain_type_kwargs\n\u001b[1;32m    117\u001b[0m )\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chains/question_answering/chain.py:265\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m     )\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loader_mapping[chain_type](\n\u001b[1;32m    266\u001b[0m     llm, verbose\u001b[38;5;241m=\u001b[39mverbose, callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    267\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain/chains/question_answering/chain.py:75\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_stuff_chain\u001b[39m(\n\u001b[1;32m     66\u001b[0m     llm: BaseLanguageModel,\n\u001b[1;32m     67\u001b[0m     prompt: Optional[BasePromptTemplate] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     73\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StuffDocumentsChain:\n\u001b[1;32m     74\u001b[0m     _prompt \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;129;01mor\u001b[39;00m stuff_prompt\u001b[38;5;241m.\u001b[39mPROMPT_SELECTOR\u001b[38;5;241m.\u001b[39mget_prompt(llm)\n\u001b[0;32m---> 75\u001b[0m     llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     76\u001b[0m         llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     77\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[1;32m     78\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     79\u001b[0m         callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m     80\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     81\u001b[0m     )\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StuffDocumentsChain(\n\u001b[1;32m     84\u001b[0m         llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[1;32m     85\u001b[0m         document_variable_name\u001b[38;5;241m=\u001b[39mdocument_variable_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     90\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     emit_warning()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py:111\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pydantic/main.py:212\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    211\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_validator__\u001b[38;5;241m.\u001b[39mvalidate_python(data, self_instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    214\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    218\u001b[0m         category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    219\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for LLMChain\nverbose\n  Input should be a valid boolean [type=bool_type, input_value=None, input_type=NoneType]\n    For further information visit https://errors.pydantic.dev/2.9/v/bool_type"
     ]
    }
   ],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # Retrieve top 1 relevant document\n",
    "\n",
    "# Initialize the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  # \"stuff\" is a simple chain type that stuffs all retrieved docs into the prompt\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
